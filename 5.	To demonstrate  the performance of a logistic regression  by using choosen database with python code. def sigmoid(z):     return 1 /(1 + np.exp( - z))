import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load dataset (replace this with your dataset loading code)
data = load_iris()
X, y = data.data, data.target

# For the purpose of this example, we'll consider only the first two features
X = X[:, :2]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Define logistic regression model
class LogisticRegressionModel:
    def __init__(self):
        self.weights = None

    def fit(self, X, y, learning_rate=0.01, epochs=1000):
        m, n = X.shape
        self.weights = np.zeros(n + 1)  # Additional weight for the bias term
        X = np.c_[np.ones(m), X]  # Add bias term to input features

        for _ in range(epochs):
            z = np.dot(X, self.weights)
            predictions = sigmoid(z)
            gradient = np.dot(X.T, (predictions - y)) / m
            self.weights -= learning_rate * gradient

    def predict(self, X):
        X = np.c_[np.ones(X.shape[0]), X]  # Add bias term
        z = np.dot(X, self.weights)
        return np.round(sigmoid(z))

# Initialize and train the logistic regression model
log_reg_model = LogisticRegressionModel()
log_reg_model.fit(X_train, y_train)

# Make predictions
y_pred_train = log_reg_model.predict(X_train)
y_pred_test = log_reg_model.predict(X_test)

# Calculate accuracies
train_accuracy = accuracy_score(y_train, y_pred_train)
test_accuracy = accuracy_score(y_test, y_pred_test)
print("Training Accuracy:", train_accuracy)
print("Testing Accuracy:", test_accuracy)

# Plot decision boundary
def plot_decision_boundary(model, X, y):
    # Plot data points
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')

    # Plot decision boundary
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.01),
                           np.arange(x2_min, x2_max, 0.01))
    Z = model.predict(np.c_[xx1.ravel(), xx2.ravel()])
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.8, cmap=plt.cm.coolwarm)

    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('Logistic Regression Decision Boundary')
    plt.show()

# Plot decision boundary
plot_decision_boundary(log_reg_model, X_train, y_train)
